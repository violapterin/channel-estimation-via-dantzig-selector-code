
We follow the argument outlined in ``The Dantzig Selector'' and fill the content.
The sparse vector to be recovered of our investigation is \(g \in \MB{C}^{N_g}\), and we continue to use \(\hat{g}\) to denote the Dantzig Selector.
To simplify the exposition, we will continue to use the variables \(\V{P}\), \(\V{g}\), \(\V{y}\), and \(\V{z}\) introduced in previous sections, without defining them over and over (by which one would make each statement completely self-containing but considerably more verbose).

We first establish a technical lemma that shows \(\RM{supp}(g)\) has roughly cardinal \(s\).
At the heart of the argument, the lemma provides the basis on which \(g\) is almost-sparse.

\subsection{Almost-Sparsity of Beamspace Channel Matrix}

We apply DS to find \(\hat{g}\), setting
\Disp{
\gamma =\sqrt{2 \log N_g}
}
Other values are of course possible, but this is enough to illustrate our purpose.
Also set for short
%
\Disp{
d =\hat{g} -g
}
%
And call
\begin{itemize}
\item \(\MC{A}\) be the largest \(s\) position of \(g\).
\item \(\MC{B}\) be the largest \(s\) position of \(d_{T(N_g) -\MC{A}}\).
\item \(\MC{C} =T(N_g) -\MC{A}\), i.e., the complementary index set of \(\MC{A}\).
\end{itemize}

Recall the definition () in section 1.

\Result
{Definition}
{
Suppose \(\M{A} \in \MB{M}_{\MB{K}} (N_1, N_2)\).
Let \(\MC{T} \subset T (N_2)\).
Denote as \(\M{A}_{\MC{T}}\) the columns of \(\M{A}\) having indices in \(\MC{T}\).
}

\Result
{Definition}
{
The function \(S:\; \MB{V}_{\MB{K}}(N) \mapsto \MB{V}_{\MB{K}}(N)\), \(N \in \MB{N}\), sorts the vector \(x \in \MB{V}_{\MB{K}}(N))\), so that
\Disp{
|S (x)(n_1)| \geq &|S (x)(n_2)|, \\
n_1 >&n_2,
\quad n_1, n_2 \in T (N) \NT
}
}

Note that there are infinitely many sorting functions depending on how to break the tie, but we fix any \(S\) throughout this article.
For definiteness, we may compare the first component first, and the second, and so on.
If still there is a tie for a certain component (for complex numbers), we compare the real part first, then the imaginary part.


\Result
{Definition}
{
Let function \(f:\; T (N) \mapsto \MB{R}_+\), \(N \in \MB{N}\), be given.
For \(x \in \MB{V}_{\MB{K}}(N)\), we say \(x\) is confined in sorted magnitude according to \(f\), if
\Disp{
|S(x) (n)| <&f (n),
\quad n \in T (N).
}
}

\Result
{Definition}
{
For \(x \in \MB{V}_{\MB{K}}(N)\), we say \(x\) is almost-\(s\)-sparse with \(\ell_1\)-residue \(R\), where \(s \ll N\), if
\Disp{
\sum_{n=s+1}^N |S(x) (n)|
\leq R.
}
}


\Result
{Lemma (Tentative)}
{
Suppose \(0 \leq \varphi \leq 2\pi k /N_H\), \(k =0, \dotsc, N_H-1\).
Let \(\varphi\) be given, and array response \(\V{a} (\varphi)\) defined as in ().
Then \(\V{a} (\varphi)\) is almost-\(L\)-sparse with \(\ell_1\)-residue \(R\)
%
\Disp{
R
\leq 2 N_H \log \F{2\pi N_H}{L}.
}
}

The lemma is the key to estimating sparsity of \(h\), which we now show.
Recall the identity for Dirichlet kernel
%
\Disp{
D (\varphi')
:=&\left| \sum_{n=0}^{N_H-1} \RM{e}^{i n \varphi'} \right| \NT \\
=&\F{|\sin (N_H \varphi'/2)|}{|\sin (\varphi' /2)|},
\quad 0 \leq \varphi' \leq \pi
}
%
And recall the Taylor expansion of sin for the first two terms with respect to zero
%
\Disp{
\sin x =x -\F{x^3}{6} + \MC{O}(x^5), \quad x \ll 1
}
%
Actually it can be shown that we have a inequality
%
\Disp{
\left| x -\F{x^3}{6} \right| \leq \sin x, \quad -\pi \leq x \leq \pi
}
%
Applying the above inequality to the denominator and bounding the nominator by 1, we have
%
\Disp{
\left| D (\varphi') \right|
= \F{48}{|\varphi'^2 -24| |\varphi'|}
\quad 0 \leq \varphi' \leq \pi,
}

Particularly, for arbitrary \(\varphi\), find \(k\) so that
\Disp{
\eta :=&\varphi -\F{2\pi k}{N_H}, \\
\left| \eta \right|
\leq &\F{2\pi}{N_H}.
}
%
Then observe
\Disp{
K^\H a(\varphi) (k)
=D \left( \varphi -\F{2 \pi k} {N_H} \right)
}
So we need only bound
\Disp{
R(\eta) :=\sum_{n_H =s}^{N_H -1} D \left( \eta +\F{2 \pi n_H} {N_H} \right)
}
By the approximation of rectangle to integral,
\Disp{
R(\eta) -\F{2\pi} {N_H}
\leq N_H \int_{\varphi'=2\pi L/N_H}^{2\pi} |D(\varphi')| d \varphi'
}
Bound \(D\) by above technique, meanwhile we notice \(\varphi' <24\).
\Disp{
R(\varphi)
\leq &\F{N_H}{2\pi} \int_{\varphi'=2\pi L/N_H}^{2\pi} \F{48}{(24 -\varphi'^2) \varphi'} d \varphi'
+\F{2\pi} {N_H} \\
=&\F{N_H}{\pi} \log \F{2\pi N_H}{L} -\log \F{4\pi^2 -24} {L^2/N_H^2 -24}
+\F{2\pi} {N_H}
}
Notice the second log term \(<1\) and we drop that.
Also drop \(2\pi /N_H\) since it is small.
Furthermore the \(2\pi\) factor in log can be dropped too.
\Disp{
R(\varphi)
\leq \F{N_H}{\pi} \log \F{N_H}{L}.
}

\Result
{Lemma (Tentative)}
{
Let \(\varphi\) and \(\vartheta\) be uniformly, independently distributed in \([0,2\pi)\), and linear array response \(\V{a}\) defined as in ().
Let \(g\) be defined as above.
Then \(g\) is almost-\(L\)-sparse with \(\ell_1\)-residue
%
\Disp{
R
\leq \F{L N_H^2}{\pi^2} \left( \log \F{N_H}{L} \right)
}
}

\subsection{Design of RIP Precoders and Combiners}

\Result
{Definition}
{
For \(\MC{T}, \MC{T}' \subset \{1, \dotsc, N_p\}\), define the \(s,s'\)-restricted orthogonality constant \(\tau_{s,s'} >0\) to be the smallest such that
%
\Disp{
| \IP{ P_{\MC{T}} h, P_{\MC{T}'} h' } |
\leq \tau_{s, s'} \cdot \| h \|_2 \|h'\|_2
}
}

From ``Decoding from Linear Programming'' (Cand\`es and Tao 2005), Lemma 1.1:

\Result
{Lemma}
{
Let \(P\) be given.
Then \(\tau_{s, s'}\) is bounded in both direction as follows,
\Disp{
\delta_{s+s'} -\max (\{\delta_s, \delta_{s'}\})
\leq \tau_{s, s'}
\leq \delta_{s+s'}
}
}

Thus \(\delta_s\), which defines how much the deformation of norm is, also tells us how much the inner product is deformed.
We may well keep track of \(\delta_s\) only.

\Result
{Lemma}
{
Let \(P\) be i.i.d.\ Radmacher, as defined in Lemma ().
Then \(P\) is RIP according to \(\delta\) for at least probability
\Disp{
   1 -2 \left( \F{12}{\delta} \right)^s \exp \left[ - \left( \F{\epsilon^2}{4} -\F{\epsilon^3}{6} \right) \F{\delta}{2} N_p \right]
}
}


{ \color{red} (To be done) }

\subsection{DS for Complex Vectors}

The rest follows ``The Dantzig Selector'' very closely.
The generalization to complex vector is necessary (though we will ultimately do it in simulation involving real numbers only.
We spell out the proof when such generalization is nontrivial.

\Result
{Proposition}
{
Let \(g\) and \(d\) be defined as above.
Then
%
\Disp{
\|d_{\MC{C}}\|_1
\leq \|d_{\MC{A}}\|_1 +2\|g_{C}\|_1
}
}

To show this, observe that with triangle inequality applied respectively on \(\MC{A}\) and \(\MC{C}\),
\Disp{
\|g\|_1
-\|d_{\MC{A}}\|_1
+\|d_{\MC{C}}\|_1
-\|g_{\MC{C}}\|_1
\leq \|g+d\|_1
}
In the first line, we recall \(g =g_{\MC{A}}\).
On the other hand, by construction that \(\hat{g}\) minimizes the \(\ell_1\)-norm,
\Disp{
\|g+d\|_1
=\|\hat{g}\|_1
\leq \|g\|_1
}
The result follows by combining.

\Result
{Lemma}
{
Let \(z\) and \(P\) be defined as above.
Then
%
\Disp{
|\IP{ z, P(:, n_g) }|
\leq& 2 \sqrt{\log N_g},
\quad n_g =1, \dotsc, N_g.
}
%
with overwhelming probability.
}

This only depends on the fact that \(P(:, n_g)\) has unity \(\ell_2\)-norm, and on the randomness of \(z\).

{ \color{red} (To be done) }

\Result
{Lemma}
{
Let \(d\) and \(P\) be defined as above, and \(P\) satisfies RIP as in ().
%
\Disp{
\|P^\H P d\|_\infty
\leq& 4 \sqrt{\log N_g},
\quad n_g =1, \dotsc, N_g.
}
}

To show this, by definition
%
\Disp{
&\IP{ z -(y -P \hat{g}), P (:,n_y) } \NT \\
=&\IP{ P \hat{g} -P g, P (:,n_y) } \NT \\
=&\IP{ P d, P (:,n_y) }
}

By construction
\Disp{
\| P (:,n_y)^\H (y -P \hat{g}) \|
\leq &\| P^\H (y -P \hat{g}) \|_\infty \\
\leq &2\sqrt{\log N_g}
}
By triangle inequality, together with lemma (), we have
\Disp{
\|P (:,n_y)^\H P d\|
\leq &\|\IP{ z, P (:,n_y) }\| +\IP{ y -P \hat{g}, P (:,n_y) } \\
\leq &2 \sqrt{\log N_g} +2 \sqrt{\log N_g}
}
which implies
\Disp{
\|P^\H P d\|_\infty
\leq &4 \sqrt{\log N_g}.
}

From ``Dantzig Selector'' (Cand\`es and Tao 2007), Lemma 1, first equation, we have the result as below.
The original result is for real vector spaces, but we have checked that the proof is completely valid in complex vector spaces.
Of course the interpretation of modulus and the inner product changes accordingly.

\Result
{Lemma}
{
Let \(d\) and \(P\) be defined as above, and \(P\) satisfies RIP as in () with respect to \(\delta_s\).
%
\Disp{
\|d_{\MC{AB}}\|_2
\leq \F{1}{1-\delta_{2s}} \|P_{\MC{A}\MC{B}}^\Tr P d\|_2 +\F{\delta_{3s}}{(1-\delta_{2s}) \sqrt{L}} \|d_{C}\|_1
}
%
}

From ``Dantzig Selector'' (Cand\`es and Tao 2007), Lemma 1, second equation, we have the result as below.
Again, their proof works with complex vector spaces in place of real ones too.

\Result
{Lemma}
{
Let \(d\) be defined as above.
Then
%
\Disp{
\|d\|_2^2
\leq \|d_{\MC{A}\MC{B}}\|_2^2 +\F{1}{L} \|d_{\MC{C}}\|_1^2
}
%
}

\subsection{The Main Bound}

We can now formulate and prove the main bound of ours.

\Result
{Theorem}
{
Let \(y\), \(P\), \(g\), \(\hat{g}\), \(d\) be defined as above.
Then, if \(g\) is almost-\(L\)-sparse with residue \(\epsilon(N,L)\), then
\Disp{
-\|g_{\MC{C}}\|_2
\leq C 4 (\log N_g) \cdot \left( L +\F{1}{L} \epsilon(N,L) \right)
}
with overwhelming probability.
}

\Disp{
\| d \|_2^2
\leq &\|d_{\MC{A}\MC{B}}\|_2^2 +\F{1}{L} \|d_{\MC{C}}\|_1^2 \\
\leq \F{1}{(1-\delta_{2s})^2}
\left(
\|P_{\MC{A}\MC{B}}^\Tr P d\|_2^2
+\F{\delta_{3s}^2} {(1-\delta_{2s})^2 L} \|d_{C}\|_1^2
\right)
+\F{1}{L} \|d_{\MC{C}}\|_1^2
}


{ \color{red} (To be done) }
