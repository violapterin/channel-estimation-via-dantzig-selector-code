\startchapter [title={Theorems and Proofs}]

Our plan is first to show that \m {\SB {\V {g}}} is almost sparse, and substitute the bound into the expected square error of DS, thus generalizing the original argument.
We will continue to use the variables \m {\V {P}}, \m {\V {g}}, \m {\V {y}}, and \m {\V {z}} introduced in previous sections.

\startsection [title={Almost-Sparsity of Array Response \m {\V{a}}}]

Split \m {\V{g}} into two parts: \m {\V {g} _\MC {A}} is the largest \m {S} positions of \m {\V{g}}, \m {\V {g} _\MC {A}} the next \m {S} largest positions of \m {\V{g}}, and \m {\V {g} _\MC {K}} are the positions complement to \m {\V {g} _\MC {A}}.
For example, if \m {\V {g} =\IP {-1,3,-4,2,8}}, and \m {S=2}, then \m {\V {g} _\MC {A} =\IP {0,3,0,0,8}}, \m {\V {g} _\MC {B} =\IP {-1,0,0,2,0}}, and \m {\V {g} _\MC {A} =\IP {-1,0,-4,2,0}}.
We rephrase it formally.

\Result
{Definition}
{
Consider \m {\V{x} \in \MB {V}_{\MB {K}} \SB {N_p}}.
Let bijection \m {\s: \MC {T} \SB {N_p} \mapsto \MC {T} \SB {N_p}} be defined to sort \m {\V{x}} in decreasing magnitude, meaning
\Disp{
\NC \V{v} \DB {\s \SB {n}}
\leq \NC \V{v} \DB {\s \SB {n-1}} \NR[+]
\NC n =\NC 1, \ldots, N_p-1 \NR
}
Its existence is evident.

For fixed \m {S}, define
\Disp{
\NC \MC {S} \SB {\V{x}, S}
=\NC \sum_{i=0} ^{S-1} \V {x} \DB {\s \SB{i}} \V {u} _{\s \SB{i}} \NR[+]
\NC \MC {C} \SB {\V{x}, S}
=\NC \sum_{i=S} ^{N_p} \V {x} \DB {\s \SB{i}} \V {u} _{\s \SB{i}} \NR[+]
}
}

Now set
\Disp{
\NC \V{g} _{\MC {A}}
=\NC \MC {S} \SB {\V{g}, S} \NR[+]
\NC \V{g} _{\MC {K}}
=\NC \MC {C} \SB {\V{g}, S} \NR[+]
\NC \V{g} _{\MC {B}}
=\NC \MC {S} \SB {\V{g} _\MC {K}, S} \NR[+]
}
We hope that \m {\VNm {\V{g} _\MC {K}}_1} is small.

\Result
{Definition}
{
For \m {\V {x} \in \MB {V}_{\MB {K}} \SB {N}}, we say \m {\V {x}} is almost-\m {s}-sparse according to the \m {\ell_p}-norm with remainder \m {R}, if
\Disp {
\NC \VNm {\MC {C} \SB{\V {x}, s}} _p
\leq \NC R. \NR[+]
}
}

Let \m {\f} be fixed.
It suffices to bound
\Disp {
\NC \NC \VNm {
   \MC {C} \SB {\M {K}^\Adj \V {a} \SB {\f}, s}
} _2 ^2 \NR[+]
}

Towards that end, introduce
\Disp {
\NC \psi \SB {\f, n_H}
:=\NC \RB {
   \RB {
      \RB {\f \; \Rm{Mod}\; \F{2\pi}{N_H}}
      +\F {2 \pi n_H} {N_H}
      +\pi
   } \;
   \Rm{Mod}\; \RB {2\pi}
}
-\pi \NR
}
Note that by construction
\Disp{
\NC \Nm {\psi \SB {\f, n_H}}
\leq \NC \pi \NR
}
And define the so-called Dirichlet kernel
\Disp {
\NC D \SB {\psi'}
:= \NC \sum_{n_H=0}^{N_H-1} \Ss {e}^{i n_H \psi'} \NR
}
Then observe
\Disp {
\NC \RB {\M {K}^\Adj \V {a} \SB {\f}} \DB {n_H}
=\NC \F {1}{N_H} D \SB {\psi \SB {\f, n_H}} \NR
}

Now, it can be verified that
\Result
{Lemma}
{
For \m {-\pi \leq x < \pi}, we have
\Disp {
\NC \Nm {x - \F {x^3}{6}} \leq \NC \Nm {\sin x}. \NR[+]
}
}

Applying Lemma () to the denominator of () and bounding the nominator by 1, we have
\Disp {
\NC \Nm {D \SB {\psi'}}
= \NC \F {\Nm {\sin \SB {N_H \psi'/2}}}{\Nm {\sin \SB {\psi' /2}}} \NR
\NC \leq \NC B \SB {\psi'} \NR
\NC := \NC \F {48}{\Nm {\psi'^2 -24} \Nm {\psi'}} \NR[+]
\NC -\pi \leq \NC \psi' < \pi. \NR
}
Thus,
\Disp {
\NC R^2
=\NC \F {1}{N_H^2}
\VNm {
\MC {C}
\SB{
   \sum_{n_H' =0}^{N_H -1}
   B \SB {\psi \SB {\f, n_H'}}
   \V {u}_{n_H'},
   s
}
} _2^2
\NR[+]
}
Note that \m {\Nm {B \SB {\psi'}}} is strictly decreasing in \m {\SB {0,\pi}}.
We seek to bound the \quotation {rectangulars} with an integral, and we have to split the cases that \m {N_H} is odd and even.
Anyway, a moment's reflection shows
\Disp {
\NC R^2
\leq \NC \F {1}{N_H^2} \D \F {N_H}{2\pi} \D 2 \int_{\pi s/N_H}^{\pi} B \SB {\psi'} ^2 d \psi' \NR
\NC = \NC \F {2304} {N_H \pi^6}
\int _{s /N_H} ^1 \F{1} {(24/\pi^2 -x'^2)^2 x'^2} dx' \NR
\NC =\NC \F{1} {2\pi^2 N_H}
\RB {
  -\F {8} {u}
+\F {4\pi^2 u} {24 -\pi^2 u^2}
+\R{6} \pi \tanh^{-1} \SB {\F {\pi u} {2\R{6}}}
}
\Bigg \| _{s/N_H} ^1 \NR
\NC \leq \NC \F {4} {\pi^2} \D \F {1} {s} +\F {0.032670} {N_H} \NR[+]
}
In the last step we bound the lower limit of the second and third term with \m {x' =0}, and leave the dominating \m {1/x'}.

Expanding \m {R} gives
\Disp {
\NC R
\leq \NC \F {2} {\pi} \D \F {1} {\R{s}} +\F {\R {s}} {38 N_H}. \NR[+]
}

Fixing \m {N_H}, it shall be easy to see that rhs is strictly decreasing in \m {N_H}, and the value of \m {s} that minimizes it is about \m {s =5N_H}.
In the range that we care, rhs is thus strictly decreasing in \m {s}.
\m {N_H}.

\Result
{Lemma}
{
Let \m {\f \SB {\o_h}} be given, and linear array response \m {\V {a} \SB {\f}} defined as in ().
Suppose \m {N_H \geq 4}.
Then, for all instances of \m {\f}, \m {\V {a} \SB {\f}} is almost-\m {s}-sparse according to the \m {\ell_2}-norm with remainder \m {R} to be
\Disp{
\NC R
\leq \NC \m {\F{1}{3 \R{N_H}}} \NR[+]
}
}

Since the bound is also decreasing in \m {N_H}, for concreteness, let us plug in \m {N_H =4} for the upper bound, giving
\Disp{
\NC R
\leq \NC \F {1} {6} \NR[+]
}

\stopsection

\startsection [title={Almost-Sparsity of \m {\M{G}}}]

It remains to bound \m {\VNm {\M {G}} _F}.
Note that
\Disp{
\NC \VNm {\M {G}}_F
=\NC \VNm {\M {H}}_F \NR[+]
}
So it suffices to consider \m {\VNm {\M {H}} _F}.

\Disp{
\NC \VNm {\sum _{l=0} ^{L-1} \a_l \V {a} \SB {\f_l} \V {a} \SB {\th_l} ^\Adj } _F
\leq \NC \sum _{l=0} ^{L-1} \Nm {\a_l}\VNm {\V {a} \SB {\f_l} \V {a} \SB {\th_l} ^\Adj } _F \NR
\NC \leq \NC
\sum _{l=0} ^{L-1}
\Nm {\a_l} \VNm {\V {a} \SB {\f_l}} _2
\VNm {\V {a} \SB {\th_l}} _2 \NR[+]
}
Lastly, we know \m {\ell_1}-norm is larger than \m {\ell_2}-norm.
Since \m {\ell_1}-norm will be used in later proof, we display the statement about \m {\ell_1}-norm.
\Disp{
\NC \NC \MB {E} \SB {\VNm {\MC {C} \SB {\sum _{l=0} ^{L-1} \a_l \V {a} \SB {\f_l} \V {a} \SB {\th_l} ^\Adj}} _F} \NR
\NC \leq \NC \F {1} {9 \R{N_H}} \MB {E} \SB {\sum _{l=0} ^{L-1} \Nm {\a_l}} \NR
\NC \leq \NC \F {\R{\pi} L} {3 \R{N_H}} \NR[+]
}

\Result
{Lemma}
{
Let \m {\f \SB {\o_h}} and \m {\th \SB {\o_h}} be uniformly, independently distributed in \m {[0,2\pi)}, and let \m {\V {g} \in \MB{V}_\MB{C} \SB{N_h}} be defined as in ().
If \m {N_h \geq 4^2 =16}, then \m {\V {g}} is almost-\m {s^2 L}-sparse with \m {\ell_1}-residue \m {R} to be
\Disp {
\NC R
\leq \NC \F {\R{\pi} L} {3 \R{N_H}} \NR
}
}

\stopsection

\startsection [title={Moments of Precoders' and Combiners' entries}]

Our plan is setting the four precoder matrices to be i.i.d.\ random matrix, hoping that the resulting \m {P} has RIP.
Indeed, \m {\d_s \SB {\M{F}_B} \SB {\o_t}} may be found, along similar lines with Achlioptas (2001) and Baraniuk et.\ al.\ (2008).
We prove a concentration inequality, as explained below, and invoke Chebyshev inequality.
But before that, we have to investigate the moments of each entry of \m {\M{F}_B \SB {\o_t}} (resp.\ \m {\M{W}_B \SB {\o_t}}) and that of \m {\M{F}_R \SB {\o_t}} (resp.\ \m {\M{W}_R \SB {\o_t}}).

Let us call it a day and set each entry of \m {\M{F}_B} to be i.i.d.\ Gaussian r.v.\ with mean 0, standard deviation \m {1/2}, multiplied by a normalizing constant \m {\l_B >0}.
They are (with a slight of abuse of meaning of \m {\o_t} and so on)
\Disp{
\NC d \o_t
=\NC \F {1} {\R {\pi} \l} \exp
  \RB {-\F{1}{\l^2} \MF{Re} \SB {\M{F}_B \DB {n_R, n_Y}} ^2}
d \MF{Re} \SB {\M{F}_B \DB {n_R, n_Y}} \NR[+]
\NC d \o_t
=\NC \F {1} {\R {\pi} \l} \exp
  \RB {-\F{1}{\l^2} \MF{Im} \SB {\M{F}_B \DB {n_R, n_Y}} ^2}
d \MF{Im} \SB {\M{F}_B \DB {n_R, n_Y}} \NR[+]
\NC n_R
= \NC 0, 1, 2, \ldots, N_R -1 \NR
\NC n_Y
= \NC 0, 1, 2, \ldots, N_Y -1 \NR
}

We know that the magnitude \m {\M{F}_B \DB {n_R, n_Y}} follows Rayleigh distribution, having
\Disp{
\NC \NC \MB{E} \SB {\Nm {\M{F}_B \DB {n_R, n_Y} \SB {\o_t}}} \NR
\NC =\NC \F{\R{\pi}}{2} \l_B \NR[+]
\NC M_{B,2}
:=\NC \MB{E} \SB {\Nm {\M{F}_B \DB {n_R, n_Y} \SB {\o_t}}^2} \NR
\NC =\NC \l_R^2 \NR[+]
\NC M_{B,4}
:=\NC \MB{E} \SB {\Nm {\M{F}_B \DB {n_R, n_Y} \SB {\o_t}}^4} \NR
\NC =\NC 2 \l_R^4 \NR[+]
}
Also notice that
\Disp{
\NC \NC \MB{E} \SB {\M{F}_B \DB {n_R, n_Y}^2} \NR
\NC = \NC \MB{E} \SB {\MF {Re} \SB {\M{F}_B \DB {n_R, n_Y}}^2}
   -\MB{E} \SB {\MF {Im} \SB {\M{F}_B \DB {n_R, n_Y}}^2} \NR
\NC \NC \FourQ +2 \Ss {i} \MB{E} \SB {\MF {Re} \SB {\M{F}_B \DB {n_R, n_Y}}}
      \MB{E} \SB {\MF {Im} \SB {\M{F}_B \DB {n_R, n_Y}}} \NR
\NC = \NC 0 \NR[+]
}

Let \m {\M{F}_R \DB {n_H, n_R}} be uniformly distributed on the unit circle on the complex plane, which gives probability density
\Disp{
\NC d \o_t
= \NC \F {1} {\pi} \RB {1 -\RB {\MF{Re} \SB {\M{F}_R \DB {n_H, n_R}}} ^2}^{-1/2}
d \MF{Re} \SB {\M{F}_R \DB {n_H, n_R}} \NR[+]
\NC d \o_t
= \NC \F {1} {\pi} \RB {1 -\RB {\MF{Im} \SB {\M{F}_R \DB {n_H, n_R}}} ^2}^{-1/2}
d \MF{Im} \SB {\M{F}_R \DB {n_H, n_R}} \NR[+]
}
As a result,
\Disp{
\NC \NC \MB{E} \SB {\Nm {\M{F}_R \DB {n_H, n_R} \SB {\o_t}}} \NR
\NC =\NC \l_R \NR[+]
\NC M_{R,2}
:=\NC \MB{E} \SB {\Nm {\M{F}_R \DB {n_H, n_R} \SB {\o_t}}^2} \NR
\NC =\NC \l_R^2 \NR[+]
\NC M_{R,4}
:=\NC \MB{E} \SB {\Nm {\M{F}_R \DB {n_H, n_R} \SB {\o_t}}^4} \NR
\NC =\NC \l_R^4 \NR[+]
}
And again
\Disp{
\NC \MB{E} \SB {\M{F}_R \DB {n_H, n_R}^2}
= \NC 0 \NR[+]
}


\startsection [title={Confirming the Restricted Isometry of \m {P}}]

\Result
{Definition}
{
Let \m {P} be fixed.
For \m {\MC {T}, \MC {T}' \subset \MC {T} \RB {N_p}}, define the \m {s, s'}-restricted orthogonality constant \m {\tau_{s,s'} \SB {P} >0} to be the smallest number such that
\Disp {
\NC \Nm {\IP {P _{\MC {T}} h, P _{\MC {T}'} h'}}
\leq \NC \tau_{s, s'} \SB {P} \cdot \VNm {h} _2 \VNm {h'} _2 \NR
}
}

From \quotation {Decoding from Linear Programming} (Cand\`es and Tao 2005), Lemma 1.1:

\Result
{Lemma}
{
Let \m {\M{P}} be fixed.
Then \m {\tau_{s, s'} \SB {\M{P}}} is bounded in both direction as follows,
\Disp {
\NC \d_{s+s'} \SB {\M{P}} -\max \SB {\CB {\d_s \SB {\M{P}}, \d_{s'} \SB {\M{P}}}}
\leq \NC \tau_{s, s'} \SB {\M{P}} \NR
\NC \leq \NC \d_{s+s'} \SB {\M{P}} \NR[+]
}
}

Thus \m {\d_s \SB {\M{P}}}, which defines how much the deformation of norm is, also tells us how much the inner product is deformed.
We may well keep track of \m {\d_s \SB {\M{P}}} only.
When \m {\M{P}} is clear, we may suppress it.

Similar is true of \m {\d_s \SB {\M{F}_B} \SB {\o_t}}.
But each entry of \m {P} is a linear combination of products, and such reasoning does not work.
If only \m {P} were also an i.i.d.\ random matrix, the resulting proof would be easier.
Is the approach all but lost?


Observe
\Disp {
\NC \M{P} ^\Adj \M{P}
=\NC
\RB {
   \RB {\M{K} ^\Tr \M {F}_R^\ast \M {F}_B^\ast}
   \otimes \RB {\M {K} \M {W}_R \M {W}_B}
}
\RB {
   \RB {\M {F}_B^\Tr \M {F}_R^\Tr \M{K}^\ast}
   \otimes \RB {\M {W}_B \M {W}_R \M {K}}
} \NR
\NC =\NC
\RB {\M {F}_B^\Tr \M {F}_R^\Tr \M{K}^\ast \M{K}^\Tr \M {F}_R^\ast \M {F}_B^\ast}
\otimes \RB {\M {W}_B \M {W}_R \M{K} \M{K}^\Adj \M {W}_R^\Adj \M {W}_B^\Adj} \NR
\NC =\NC
\RB {\M {F}_B^\Tr \M {F}_R^\Tr \M {F}_R^\ast \M {F}_B^\ast}
\otimes \RB {\M {W}_B \M {W}_R \M {W}_R^\Adj \M {W}_B^\Adj} \NR
\NC =\NC \M{Q} ^\Adj \M{Q} \NR
}
where as we defined before \m {\M{Q} :=\RB {\M {F}_R^\Tr \M {F}_B^\Tr} \otimes \RB {\M {W}_B \M {W}_R}}.

This implies
\Disp {
\NC \V{u} ^\Adj \M{P} ^\Adj \M{P} \V{u}
= \NC \V{u} ^\Adj \M{Q} ^\Adj \M{Q} \V{u} \NR[+]
}
or
\Result
{Lemma}
{
For any instance of \m {\M{P} \SB {\o_t, \o_r}} and \m {\M{Q} \SB {\o_t, \o_r}},
\Disp{
\NC \VNm {\M{P} \V{u}} _2
= \NC \VNm {\M{Q} \V{u}} _2 \NR[+]
}
}

And it will turn out that any two distinct entries of \m {\M{Q}} are not correlated.

\Result
{Definition}
{
To make expressions more compact, introduce the indication function \m {\i}, so that, for some injective \m {\s:\; \CB {0, \ldots, M-1} \CB {0, \ldots, N-1}},
\Disp {
\NC \i_{a_0, \ldots, a_{M-1}} \SB {x_0, \ldots, x_{N-1}}
=\NC \startcases
\NC 1, \MC \Q \exists \s:\; x_0 \cdots x_{N-1}
=x_{\s \SB {0}} ^{a_0} \cdots x_{\s \SB {M-1}} ^{a_{M-1}} \NR
\NC 0, \NC \Q \Rm {otherwise} \NR
\stopcases \NR
}
}

For short, denote
\Disp {
\NC F :=\NC \M {F}_B \M {F}_R \NR
\NC W :=\NC \M {W}_R \M {W}_B \NR
}
\Result
{Lemma}
{
\Disp {
\NC \NC \MB{E} \SB {\M{F} \DB {n_H, n_Y} \M{F} \DB {n_H', n_Y'}} \NR
\NC = \NC \MB{E} \SB {\M{W}^\Adj \DB {n_Y, n_H} \M{W}^\Adj \DB {n_Y', n_H'}} \NR
\NC = \NC \i_2 \SB {n_H, n_H'} \D \i_2 \SB {n_Y, n_Y'} \D N_R \l_R^2 \l_B^2 \NR[+]
}
}

To see this, expanding the definition
\Disp {
\NC \NC \M{F} \DB {n_H, n_Y} \M{F} \DB {n_H', n_Y'} \NR
\NC =\NC
\RB {\sum_{n_R=0}^{N_R-1} \M{F}_R \DB {n_H, n_R} \M{F}_B \DB {n_R, n_Y}}
\RB {\sum_{n_R=0}^{N_R-1} \M{F}_R \DB {n_H', n_R} \M{F}_B \DB {n_R, n_Y'}} \NR[+]
}
Examine the \m {\i_{1,1} \SB {n_H, n_H'}} factor and \m {\i_{1,1} \SB {n_Y, n_Y'}} factors.
After taking expectation, the expectation sign is distributed, and every term vanishes.
Other terms that remains are \m {\i_{2} \SB {n_H, n_H'}} and \m {\i_{2} \SB {n_Y, n_Y'}}, namely
\Disp {
\NC \NC \sum_{n_R=0}^{N_R-1}
\MB{E} \SB {\M{F}_R \DB {n_H, n_R}^2}
\MB{E} \SB {\M{F}_B \DB {n_R, n_Y}^2} \NR
\NC = \NC N_R \D M_{R,2} M_{B,2} \NR
}

By the same token,
\Result
{Lemma}
{
\Disp {
\NC \NC \MB{E}
\SB {
\M{F} \DB {n_H, n_Y}
\M{F} \DB {n_H', n_Y'}
\M{F} \DB {n_H'', n_Y''}
\M{F} \DB {n_H''', n_Y'''}
} \NR
\NC =\NC \MB{E}
\SB {
\M{W} ^\Adj \DB {n_Y, n_H}
\M{W} ^\Adj \DB {n_Y', n_H'}
\M{W} ^\Adj \DB {n_Y'', n_H''}
\M{W} ^\Adj \DB {n_Y''', n_H'''}
} \NR
\NC = \NC
\i_4 \SB {n_H, n_H', n_H'', n_H'''}
\D \i_4 \SB {n_Y, n_Y', n_Y'', n_Y'''}
\D 2 N_R^2 \l_R^4 \l_B^4 \NR
\NC \NC \FourQ +\i_{2,2} \SB {n_H, n_H', n_H'', n_H'''}
\D \i_{2,2} \SB {n_Y, n_Y', n_Y'', n_Y'''}
\D N_R^2 \l_R^4 \l_B^4 \NR[+]
}
}

A moment's reflection shows
\Result
{Lemma}
{
\Disp {
\NC \NC \MB{E} \SB {\M{P} \DB {n_h, n_y} \M{P} \DB {n_h', n_y'}} \NR
\NC = \NC \i_2 \SB {n_H, n_H'} \D \i_2 \SB {n_Y, n_Y'} \D N_R \l_R^2 \l_B^2 \NR[+]
}
}

More trickily, but by the same idea,
\Result
{Lemma}
{
\Disp {
\NC \NC \M{F} \DB {n_h, n_y}
\M{F} \DB {n_h', n_y'}
\M{F} \DB {n_h'', n_y''}
\M{F} \DB {n_h''', n_y'''} \NR
\NC = \NC
\i_4 \SB {n_H, n_H', n_H'', n_H'''}
\D \i_4 \SB {n_Y, n_Y', n_Y'', n_Y'''}
\D 4 N_R^4 \l_R^8 \l_B^8 \NR
\NC \NC \FourQ +\i_{2,2} \SB {n_H, n_H', n_H'', n_H'''}
\D \i_{2,2} \SB {n_Y, n_Y', n_Y'', n_Y'''}
\D N_R^4 \l_R^8 \l_B^8 \NR[+]
}
}

Now, fix any test vector \m {\V{u} \in \MB{V}_\MB{C} \SB{N_Y}}, we shall study the probability concentration of \m {\VNm {\M{F}_B \V{u}}_2^2} and \m {\VNm {\M{F}_B \V{u}}_2^4}.
The two moments are present in Chebyshev bound.
\Disp{
\NC \NC \MB{E} \SB {\VNm {\M{Q} \SB {\o_t, \o_r} \V{u}} _2 ^2} \NR
\NC = \NC \MB{E} \SB {
  \sum_{n_y=0}^{N_y-1}
  \Nm {\sum_{n_h=0}^{N_h-1} \M{Q} \DB {n_y, n_h} ^\ast \V{u} \DB{n_h}} ^2
} \NR
\NC = \NC \sum_{n_y=0}^{N_y-1}
  \MB{E} \SB {\Nm {\sum_{n_h=0}^{N_h-1} \M{Q} \DB {n_y, n_h} ^\ast \V{u} \DB{n_h}} ^2}
\NR
\NC = \NC
\sum_{n_y=0}^{N_y-1}
  \MB{E} \SB {
     2 \sum_{\Stack { n_h, n_h'=0 \NR n_h <n_h' }}^{N_h-1}
       \MF {Re} \SB{
         \M{Q} \DB {n_y, n_h} ^\ast \M{Q} \DB {n_y, n_h'}
         \V{u} \DB{n_h} \V{u} \DB{n_h'} ^\ast
       }
   } \NR
\NC \NC \FourQ +\sum_{n_y=0}^{N_y-1}
  \MB{E}
     \SB {\sum_{n_h=0}^{N_h-1} \Nm {\M{Q} \DB {n_y, n_h}}^2 \Nm {\V{u} \DB{n_h}}^2}
\NR
\NC = \NC
2 \sum_{n_y=0}^{N_y-1}
\sum_{\Stack { n_h, n_h'=0 \NR n_h <n_h' }}^{N_h-1}
\MF {Re} \SB{
   \MB{E} \SB {\M{Q} \DB {n_y, n_h}^\ast}
   \MB{E} \SB {\M{Q} \DB {n_y, n_h'}}
}
\V{u} \DB{n_h} \V{u} \DB{n_h'} ^\ast
\NR
\NC \NC \FourQ +\sum_{n_y=0}^{N_y-1}
   \sum_{n_h=0}^{N_h-1} \MB{E} \SB {\Nm {\M{Q} \DB {n_y, n_h}}^2} \Nm {\V{u} \DB{n_h}}^2
\NR
\NC = \NC N_y \D N_R^2 \l_B^4 \l_R^4  \D \VNm {\V{u}} _2 ^2 \NR[+]
}
Thus, we may set
\Disp{
\NC \l_B
=\NC \l_R \NR
\NC =\NC \F {1} {\R {N_Y N_R}} \NR[+]
}
To make
\Disp{
\NC \MB{E} \SB {\VNm {\M{Q} \SB {\o_t, \o_r} \V{u}} _2 ^2}
=\NC \VNm {\V{u}} _2 ^2
}

Similarly, we have to find, in advance, the second moment.
\Disp{
\NC \MB{E} \SB {\VNm {\M{Q} \SB {\o_t, \o_r} \V{u}} _2 ^4}
=\NC \MB{E} \SB {
  \RB {
    \sum_{n_R=0}^{N_R-1}
    \Nm {\sum_{n_Y=0}^{N_Y-1} \M{Q} \DB {n_R, n_Y} ^\ast \V{u} \DB{n_Y}} ^2
  } ^2
} \NR
\NC = \NC E_1 +E_2 +E_3 +E_4 +E_5 \NR[+]
}
These terms are discussed as immediately follows.
Without spelling out everything, we notice that as long as a term has a single \m {\M{Q} \DB {n_R, n_Y}} factor, it's expectation is zero.
The terms that does not vanish is a product of three numbers: How many terms, the moment involving \m {\M{Q} \DB {n_y, n_h}}, the norm of \m {\V{u}}.
We will repeatedly bound \m {N_R \D (N_R-1) /2} by \m {N_R^2 /2}, and bound \m {\Nm {\V{u}} _2} by \m {\Nm {\V{u}} _2}, and so on.
\Disp {
\NC E_1
=\NC \sum_{n_y=0}^{N_y-1} \sum_{n_h=0}^{N_h-1}
\Nm {\M{Q} \DB {n_y, n_h}} ^4
\Nm {\V{u} \DB{n_h}} ^4 \NR
\NC =\NC N_Y^2 \D 4 N_Y ^{-4} \D \VNm {\V{u}} _4 ^4 \NR
\NC \leq \NC 4 N_Y ^{-2} \VNm {\V{u}} _2 ^4 \NR[+]
}
Now we have to bound \m {\i_{1,1} \SB {\Fl {n_y/N_Y}, \Fl {n_y'/N_Y}} M_{B,2}^2} and \m {\i_{2} \SB {\Fl {n_y/N_Y}, \Fl {n_y'/N_Y}} M_{B,4}} by \m {M_{B,4}} to simplifying matters, and similar for \m {E_3} and \m {E_4}.
\Disp {
\NC E_2
= \NC \sum _{\Stack { n_y, n_y' =0 \NR n_y <n_y' }}^{N_y-1}
\sum _{n_h =0}^{N_h-1}
\Nm {\M{Q} \DB {n_y, n_h}} ^2
\Nm {\M{Q} \DB {n_y', n_h}} ^2
\Nm {\V{u} \DB{n_h}} ^4 \NR
\NC = \NC N_Y \D \F{1}{2} N_Y \RB{N_Y-1} \D N_Y ^{-4} \VNm {\V{u}} _2 ^4 \NR
\NC \NC \FourQ + \F{1}{2} N_Y \RB{N_Y-1} \D N_Y^2 \D 4 N_R ^2 N_Y ^{-4} N_R ^{-4} \VNm {\V{u}} _2 ^4 \NR
\NC \leq \NC 4 \VNm {\V{u}} _2 ^4 \NR[+]
}
\Disp {
\NC E_3
=\NC \sum _{n_y, n_y' =0}^{N_y-1}
\sum _{\Stack { n_h, n_h' =0 \NR n_h <n_h' }}^{N_h-1}
\Nm {\M{Q} \DB {n_y, n_h}} ^2
\Nm {\M{Q} \DB {n_y', n_h'}} ^2
\Nm {\V{u} \DB{n_h}} ^2
\Nm {\V{u} \DB{n_h'}} ^2 \NR
\NC \leq \NC N_Y^2 \D \F{1}{2} N_H \RB{N_H-1} \D N_Y ^{-4} \D \RB {\VNm {\V{u}} _2 ^4 -\VNm {\V{u}} _4 ^4} \NR
\NC \leq \NC 4 N_Y ^{-1} \VNm {\V{u}} _2 ^4 \NR[+]
}
\Disp {
\NC E_4
=\NC \sum _{n_y =0}^{N_y-1}
\sum _{\Stack { n_h, n_h' =0 \NR n_h <n_h' }}^{N_h-1}
\Nm {\M{Q} \DB {n_y, n_h}} ^2
\Nm {\M{Q} \DB {n_y, n_h'}} ^2
\Nm {\V{u} \DB{n_h}} ^2
\Nm {\V{u} \DB{n_h'}} ^2 \NR
\NC \leq \NC N_Y \D \F{1}{2} N_H \RB{N_H-1} \D N_Y ^{-4} \D \RB {\VNm {\V{u}} _2 ^4 -\VNm {\V{u}} _4 ^4} \NR
\NC \leq \NC 4 N_H ^2 N_Y ^{-3} \VNm {\V{u}} _2 ^4 \NR[+]
}

The last one is more complicated.
Use the shorthand
\Disp {
\NC \sum _{n_h, n_h', m_h, m_h'} ^\star
:= \NC \sum _{
   \Stack {
     0 =n_h <n_h' \NR
     0 =m_h <m_h' \NR
     n_h +m_h' =n_h' +m_h \NR
     \Fl {n_h/N_H} =\Fl {n_h'/N_H} \NR
     \Fl {m_h/N_H} =\Fl {m_h'/N_H}
   }
} ^{N_H-1}
\sum _{
  \Stack {
     0 =n_y <n_y' \NR
     \Fl {n_y/N_Y} =\Fl {n_y'/N_Y}
  }
} ^{N_y-1} \NR
\NC \V {u} ^{(1,1,1,1)} \SB {n_h, n_h', m_h, m_h'}
:=\NC \i_{1,1,1,1} \SB {n_h, n_h', m_h, m_h'}
\V{u} \DB{n_h} \V{u} \DB{n_h'} \V{u} \DB{m_h} \V{u} \DB{m_h'} \NR
}
Then observe
\Disp {
\NC \NC \sum _{n_h, n_h', m_h, m_h'} ^\star
\V {u} ^{(1,1,1,1)} \SB {n_h, n_h', m_h, m_h'} \NR
\NC \leq \NC N_h^{-2}
\sum_{n_h, n_h' =0} ^{N_h-1}
\sum_{m_h, m_h' =0} ^{N_h-1}
\V {u} ^{(1,1,1,1)} \SB {n_h, n_h', m_h, m_h'} \NR
}
This is because one of \m {n_h, n_h'} and one of \m {m_h, m_h'} may both choose to lie in other \m {\RB {N_H-1} \RB {N_H-2}} blocks (of length \m {N_H}).
Then we need a corollary that follows from \quotation {Muirhead inequality}.
\Result
{Lemma}
{
If every component of \m {\V{u} \in \MB {M}_{\MB {C}} \SB {N_h}} is considered as a free variable taking complex values, then
\Disp {
\NC \NC \sum_{n_h, n_h' =0} ^{N_h-1}
\sum_{m_h, m_h' =0} ^{N_h-1}
\Nm {\V {u} ^{(1,1,1,1)} \SB {n_h, n_h', m_h, m_h'}} \NR
\NC \leq \NC \F{1}{6} \RB {\VNm {\V{u}} _2 ^4 -\VNm {\V{u}} _4 ^4} \NR[+]
}
}

Now,
\Disp {
\NC E_5
= \NC \sum _{n_h, n_h', m_h, m_h'} ^\star
  \M{Q} \DB {n_y, n_h}
  \M{Q} \DB {n_y, n_h'}
  \M{Q} \DB {n_y', m_h}
  \M{Q} \DB {n_y', m_h'} \NR
  \NC \NC \SixQ
  \V{u} \DB{n_h}
  \V{u} \DB{n_h'}
  \V{u} \DB{m_h}
  \V{u} \DB{m_h'} \NR
\NC =\NC N_Y \D \F{1}{2} N_H \RB{N_H-1} \D \F{1}{2} N_Y \RB{N_Y-1} \D N_H^2 \D N_Y ^{-4} \NR
\NC \NC \FourQ
\D N_H^{-2} \sum_{n_h, n_h' =0} ^{N_h-1} \sum_{m_h, m_h' =0} ^{N_h-1}
\V {u} ^{(1,1,1,1)} \SB {n_h, n_h', m_h, m_h'} \NR
\NC \leq \NC \F{1}{6} N_Y ^{-1} \VNm {\V{u}} _2 ^4. \NR[+]
}

In conclusion, \m {E_4} dominates.
\Disp{
\NC \Ss{Var} \SB {\VNm {\M{P} \SB {\o_t, \o_r} \V{u}} _2 ^2}
\leq \NC \RB {4 N_H^2 N_Y^{-3} -1} \VNm {\V{u}} _2 ^4 \NR[+]
}
Thus, by Chebyshev bound,
\Result
{Theorem}
{
Let \m {\M{P} \in \MB {M}_{\MB {C}} \SB {N_R, N_Y}} be generated randomly according to ().
Then, for any fixed \m {\V{u} \in \MB {V}_{\MB {C}} \SB {N_Y}}, and for any \m {\e >0},
\Disp{
\NC \MB{P}
\SB {
  \Nm {\VNm {\M{P} \V{u}} _2 ^2 -\VNm {\V{u}} _2 ^2}
  \geq \e \VNm {\V{u}} _2 ^2
}
\leq \NC \F {1} {\e^2} \RB {4 N_H^2 N_Y^{-3} -1} \NR[+]
}
}

According to Lemma 5.1 in Baraniuk et.\ al.\ (2008), substitution of relevent quantities yields
\Result
{Lemma}
{
Suppose the probability, according to event space \m {\o_t, \o_r}, that RIP holds for \m {\M{P}} w.r.t.\ \m {\d_s} is \m {p}, then
\Disp{
\NC 1 -p
\leq \NC 4 \D 12^s \D \d_s^{-\RB {s+2}} N_H^2 N_Y^{-3} \NR[+]
}
}

\stopsection

\startsection [title={Expected Error Analysis for Complex-Case Dantzig Selector}]

Let \m {\hat {\V {g}}} be the Dantzig Selector.
For concreteness, we set
\Disp {
\NC \g 
= \NC \R {2 \log N_h} \NR[+]
}

Other values are of course possible, but this is enough to illustrate our purpose.
Also set for short
\Disp {
\NC \V {d} 
= \NC \Hat {\V {g}} -\V {g} \NR[+]
}

The rest follows \quotation {The Dantzig Selector} very closely.
The generalization to complex vector is necessary in our setting.
We spell out the proof when such generalization is nontrivial.

\Result
{Lemma}
{
Let \m {\V {g}} and \m {\V {d}} be defined as above.
Then
\Disp {
\NC \VNm {\V {d} _{\MC {K}}} _1
\leq \NC \VNm {\V {d} _{\MC {A}}} _1 +2\VNm {\V {g} _{\MC {K}}} _1 \NR
}
}

To show this, observe that with triangle inequality applied respectively on \m {\MC {A}} and \m {\MC {K}},
\Disp {
\NC \VNm {\V {g}} _1
-\VNm {\V {d} _{\MC {A}}} _1
+\VNm {\V {d} _{\MC {K}}} _1
-\VNm {\V {g} _{\MC {K}}} _1
\leq \NC \VNm {\V {g} +\V {d}} _1 \NR
}
In the first line, we recall \m {\V {g} =\V {g} _{\MC {A}}}.
On the other hand, by construction that \m {\hat {g}} minimizes the \m {\ell_1}-norm,
\Disp {
\NC \VNm {\V {g} +\V {d}} _1
=\NC \VNm {\hat {\V {g}}} _1 \NR
\NC \leq \NC \VNm {\V {g}} _1 \NR
}
The result follows by combining.

\Result
{Lemma}
{
\Disp {
\NC \MB {E} \SB {\Nm {\IP {\V {z} \SB {\o_z}, \M {P} \DB {:, n_h}}}}
\leq \NC  2 \R {\log N_h} \NR
\NC n_h 
=\NC 1, \ldots, N_h. \NR
}
with probability \m {\geq 1 -N_h^{-1}}.
}

Indeed, recall the fact that \m {\M {P} \DB {:, n_g}} has unity \m {\ell_2}-norm, and on the randomness of \m {\V {z}}.

Recall the following bound for \m {Q} function
\Disp {
\NC Q\SB {x}
\leq \NC \F {1}{2} \Ss {e}^{-x^2/2} \NR
}
Particularly, for \m {x =\R {2 \log N_h}},
\Disp {
\NC Q\SB {\R {2 \log N_h}}
=\NC \F {1}{N_h}. \NR
}

\Result
{Lemma}
{
With the condition that \m {\M {P}} satisfies RIP for \m {\d_S \SB {\M{P}}},
\Disp {
\NC \VNm {\M {P}^\Adj \M {P} \V {d}} _\infty
\leq \NC  4 \R {\log N_h} \NR
}
with probability \m {\geq 1 -N_h^{-1}}.
}

To show this, by definition
\Disp {
\NC \IP {\V {z} -\RB {\V {y} -\M {P} \hat {\V {g}}}, \M {P} \DB {:,n_y}}
= \NC \IP {\M {P} \hat {\V {g}} -\M {P} \V {g}, \M {P} \DB {:,n_y}} \NR
\NC = \NC \IP {\M {P} \V {d}, \M {P} \DB {:,n_y}} \NR
}
By construction
\Disp {
\NC \VNm {\M {P} \DB {:,n_y}^\Adj \RB {\V {y} -\M {P} \hat {\V {g}}}} _\infty
\leq \NC \VNm {\M {P}^\Adj \RB {\V {y} -\M {P} \hat {\V {g}}}} _\infty \NR
\NC \leq \NC 2\R {\log N_h} \NR
}
By triangle inequality, together with Lemma (), we have
\Disp {
\NC \VNm {\M {P} \DB {:,n_y}^\Adj \M {P} \V {d}} _\infty
\leq \NC \VNm {\IP {\V {z}, \M {P} \DB {:,n_y}}} _\infty
+\IP {\V {y} -\M {P} \hat {\V {g}}, \M {P} \DB {:,n_y}} \NR
\NC \leq \NC 2 \R {\log N_h} +2 \R {\log N_h} \NR
}
which implies
\Disp {
\NC \VNm {\M {P}^\Adj \M {P} \V {d}} _\infty
\leq \NC 4 \R {\log N_h}. \NR
}

From \quotation {Dantzig Selector} Lemma 1, first equation, we have the result as below.
The original result is for real vector spaces, but we have checked that the proof is completely valid in complex vector spaces.
The only thing that has to change accordingly is the interpretation of magnitude and the inner product, which are evident.

\Result
{Lemma}
{
With the condition that \m {\M {P}} satisfies RIP for \m {\d_S \SB {\M{P}}},
\Disp {
\NC \VNm {\V {d} _{\MC {AB}}} _2
\leq \NC \F {1}{1-\d_{2S}} \VNm {P _{\MC {A} \MC {B}}^\Tr P d} _2 +\F {\d_{3S}}{\RB {1-\d_{2S}} \R {S}} \VNm {d_{\MC {K}}} _1 \NR
}
}

From \quotation {Dantzig Selector}, Lemma 1, second equation, we have the result as below.
Again, their proof works with complex vector spaces in place of real ones too.

\Result
{Lemma}
{
\Disp {
\NC \VNm {\V {d}} _2^2
\leq \NC \VNm {\V {d} _{\MC {A} \MC {B}}} _2^2 +\F {1}{S} \VNm {\V {d} _{\MC {K}}} _1^2 \NR
}
}

\stopsection

\startsection [title={The Main Result}]

We are going to combine previous lemmata and show the main result.
In accordance with (), set
\Disp{
\NC S
=\NC s^2 L \NR
}

\Result
{Theorem}
{
Let \m {\V {y}}, \m {\M {P}}, \m {\V {g}}, \m {\hat {\V {g}}}, \m {\V {d}} be defined as above.
Then, with \m {S =s^2 L}, as \m {N_H \to \infty},
\Disp {
\NC \VNm {\V {d}} _2
\leq \NC 32 s^2 L \log N_H \NR[+]
}
Under the design condition
\Disp {
\NC \RB {2 N_H}^2
\approx \NC N_Y^3 \NR[+]
}
the bound is true for probability \m {p}, with
\Disp {
\NC 1 -p
\leq \NC 16 \D {12}^S \d_{S}^{-S-2} N_H^2 N_Y^{-3} \NR[+]
}
}

The first quantity we want to bound away is
\Disp {
\NC \VNm {\M {P} _{\MC {A} \MC {B}}^\Tr \M {P} \V {d}} _2
\leq \NC \VNm {\M {P}^\Tr \M {P} \V {d}} _2 \NR
\NC \leq \NC \R {S} \VNm {\M {P}^\Tr \M {P} \V {d}} _\infty \NR
\NC \leq \NC 4 \R {S \log N_h} \NR
}
Thus, by virtue of Lemma (), and approximating \m {1 /(1 -\d_{2S}) =1 +\d_{2S}} and so on,
\Disp {
\NC \VNm {\V {d}_{\MC {A}}} _1
\leq \NC \R {S} \VNm {\V {d}_{\MC {A}}} _2 \NR
\NC \leq \NC \R {S} \VNm {\V {d} _{\MC {AB}}} _2 \NR
\NC \leq \NC \d_{3S} \RB {1+\d_{2S}} \VNm {\V {d} _{\MC {K}}} _1
+4 \RB {1+\d_{2S}} S \R {\log N_h} \NR
}

Next, we must eliminate \m {\VNm {d _{\MC {A}}} _1} in rhs of Lemma ().
\Disp {
\NC \VNm {\V {d} _{\MC {K}}} _1
\NC \leq \d_{3S} \RB {1+\d_{2S}} \VNm {\V {d} _{\MC {K}}} _1
+4 \RB {1+\d_{2S}} S \R {\log N_h}
+2 \VNm {\V {g} _{\MC {K}}} _1 \NR
}
Arranging and drop second order small terms like \m {\d_{2S} \d_{3S}}, that is,
\Disp {
\NC 1 \ll \NC S \NR
\NC \ll \NC N_h \NR
}
doing this we have
\Disp {
\NC \VNm {\V {d} _{\MC {K}}} _1
\leq \NC 2 \RB {1+\d_{3S}} \VNm {\V {g} _{\MC {K}}} _1
+4 \RB {1+2\d_{3S}} S \R {\log N_h} \NR
}

We are now in a position to bound \m {\VNm {d} _2^2}.
Using Lemma () again, and only keeping to first order, we have
\Disp {
\NC \VNm {\V {d}} _2^2
\leq \NC 16 S \log N_h
+8 \d_{3S} \R {\log N_h} 
+\F {\d_{3S}^2} {S} \VNm {\V {d} _{\MC {K}}} _1 \NR
}
Finally, plug in the bound for \m {\VNm {\V {d} _{\MC {K}}} _1}, the last unknown, and we make use of the almost-sparsity condition of \m {\VNm {\V {d} _{\MC {K}}} _1}.
Again for simplicity, we keep only the first order small terms.
Plug in \m {S =s^2 L} and \m {N_h =N_H^2} we get, finally,
\Disp {
\NC \VNm {\V {d}} _2^2
\leq \NC 32 s^2 L \log N_H
+\F {8 \R {2\pi}} {3} \d_{3S} L \R {\F {\log N_H} {N_H}}
+\F {\pi} {9} \F {L} {s^2 N_H} \NR
}
In particular for asymptotic \m {N_H \gg 1}, keep only the dominating term.
\Disp {
\NC \VNm {\V {d}} _2^2
\leq \NC 32 s^2 L \log N_H \NR
}

The probability that it is true may be bounded by plugging the probability in Lemma () and Lemma ().

\stopsection

\stopchapter
