\startchapter [title={Simulation}]

With the preperation of notation in the previous section, and a discussion on its theory, we shall now formulate our algorithm and focus on the practical aspect.

As of now, we have transformed the sparse-recovery problem of complex matrix \m {H} to complex vector \m {h}, and finally to real vector \m {\MC {R} \SB {h}}.
But it is not the end.
In fact, Cand\`es and Tao's analysis in ``\m {\ell_1}-MAGIC'' transforms DS into a linear program (LP), but that does not directly applies
Actually they pointed out that complex DS may be cast into a second order cone program (SOCP), but (as they admit in the paper) they did not pursue the matter.
Since the proof is, for the present purpose, not entirely trivial, we include here for completeness of exposition.
Thus our proposed algorithm is not a corollary of the DS program outlines in ``\m {\ell_1}-MAGIC''.

\startsection [title={Representation by Real Matrices}]

Since our setting is slightly different from ``\m {\ell_1}-MAGIC'', for completeness of exposition we write down derivation of the dual problem from the primal problem, as follows.
Recapitulate that, as we explained in the above, DS involves the calculation of \m {\ell_1}-1 norm.
This, for real vectors, can be transformed into a linear program (LP).
But in the complex case, that becomes a quadratic expression of the real and imaginary part of the vector, and is no longer equivalent to an LP.
Still, the comparison arising from primal-dual problem is not easily done in complex numbers, and we cast it into a real \m {\ell_1}-1 norm problem.
Afterwards we will further transform the SOCP to a primal-dual algorithm, which is a well-known technique for convex optimization problems.
The bound in the previous section is valid, since in the course of transformation, nothing is lost or weakened.

For convenience, define the following real representation function for complex vectors.

\Result
{Definition}
{
For \m {\V{x} \in \MB {V}_{\MB {C}} \SB {M}}, we say that \m {\MC {R} \SB {\V{x}}} is the real representation of \m {\V{x}}, defined by
\Disp {
\NC \MC {R} \SB {\V{x}}
\in \NC \MB {V}_{\MB {R}} \SB {2M} \NR
\NC \MC {R} \SB {\V{x}} \DB {m}
= \NC \startcases
\NC \MF {Re} \SB {\V{x} \DB {m'}}, \MC m =2m' \NR
\NC \MF {Im} \SB {\V{x} \DB {m'}}, \MC m =2m'+1 \NR
\stopcases \NR[+]
\NC m' 
= \NC  0, 1, 2, \ldots, M-1 \NR
}
The bijectivity is obvious, and we define \m {\MC {R} ^{-1}} so that
\Disp {
\NC \MC {R} ^{-1} \SB {\MC {R} \SB {\V{x}}}
=\NC \V{x} \NR[+]
}
}

By the same token, a moment's reflection reveals the corresponding definition of real representation of matrices.

\Result
{Definition}
{
For \m {\M{A} \in \MB {M}_{\MB {C}} \SB {M_1, M_2}}, we say that \m {\MC {R} \SB {\M{A}}} is the real representation of \m {\M{A}}, defined by
\Disp {
\NC \MC {R} \SB {\M{A}}
\in \NC \MB {M}_{\MB {R}} \SB {2M_1, 2M_2} \NR
\NC \MC {R} \SB {\M{A}} \DB {m_1,m_2} =
\NC \startcases
\NC \MF {Re} \SB {\M{A} \DB {m_1',m_2'}}, \MC (m_1, m_2) = (2m_1'-1, 2m_2'-1) \NR
\NC \MF {Im} \SB {\M{A} \DB {m_1',m_2'}}, \MC (m_1, m_2) = (2m_1', 2m_2'-1) \NR
\NC -\MF {Im} \SB {\M{A} \DB {m_1',m_2'}}, \MC (m_1, m_2) = (2m_1'-1, 2m_2') \NR
\NC \MF {Re} \SB {\M{A} \DB {m_1',m_2'}}, \MC (m_1, m_2) = (2m_1', 2m_2') \NR
\stopcases \NR[+]
\NC m_1 
= \NC 0, 1, 2, \ldots, M_1 -1 \NR
\NC m_2 
= \NC 0, 1, 2, \ldots, M_2 -1 \NR
}
Notice that Here not all matrices in \m {\MB {M}_{\MB {R}} \SB {2M_1, 2M_2}} are images, and we will not call the inverse function anyway.
}

We continue the notation of program ().

\Disp {
\NC \T {\V {y}}
\in \NC \MB {V}_{\MB {R}} \SB {2N_y} \NR
\NC \T {\V {y}}
= \NC \MC {R} \SB {\V {y}} \NR[+]
\NC \T {\V {g}}
\in \NC \MB {V}_{\MB {R}} \SB {2N_h} \NR
\NC \T {\V {g}}
= \NC \MC {R} \SB {\V {g}} \NR[+]
\NC \T {\M {P}}
\in \NC \MB {M}_{\MB {R}} \SB {2N_y, 2N_h} \NR
\NC \T {\M {P}}
= \NC \MC {R} \SB {\M {P}} \NR[+]
\NC \T {\M {P}} ^\Adj
\in \NC \MB {M}_{\MB {R}} \SB {2N_y, 2N_h} \NR
\NC \T {\M {P}} ^\Adj
= \NC \MC {R} \SB {\M {P} ^\Adj} \NR[+]
\NC \T {\V {z}}
\in \NC \MB {V}_{\MB {R}} \SB {2N_y} \NR
\NC \T {\V {z}}
= \NC \MC {R} \SB {\V {z}} \NR[+]
}
such that, by construction,
\Disp {
\NC \V {\T {y}}
= \NC \M {\T {P}} \V {\T {g}} +\V {\T {z}}
}

Finally, introduce the following matrices to identify the components where we want to take \m {\ell_2}-norm, in the manner similar to an indicator function.

\Disp {
\NC {\V {u}}_{n_h}
\in \NC \MB {V}_{\MB {R}} \SB {N_h} \NR
\NC {\V {u}}_{n_h} \DB {n_h'}
= \NC
\startcases
1, \Q \MC n_h' =n_h \NR
0, \Q \NC \Rm {otherwise} \NR
\stopcases \NR[+]
\NC {\T {\M {U}}}_{n_h} \in \NC \MB {M}_{\MB {R}} \SB {2N_h, 2N_h} \NR
\NC {\T {\M {U}}}_{n_h} \DB {n_h', n_h''}
= \NC
\startcases
1, \Q \MC n_h' =2n_h,\; n_h'' =2n_h \NR
1, \Q \MC n_h' =2n_h +1,\; n_h'' =2n_h +1 \NR
0, \Q \NC \Rm {otherwise} \NR
\stopcases \NR[+]
\NC n_h, n_h', n_h'' 
= \NC 0, 1, 2, \ldots, N_h -1 \NR
}
Moreover, denote \m {\V {1}} to be the all-\m{1} vector for short, and \m {\V {0}} the all-\m{0} vector, whose dimension will be specified from context.
Now, if we introduce vector \m{\V{m}}

\Disp{
\NC \V {m} \in \NC \MB {V}_{\MB {R}} \SB {N_h} \NR
\NC \V{m} \DB{n_h}
= \NC \Nm{\V{g} \DB{n_h}} \NR[+]
\NC n_h, n_h', n_h'' 
= \NC 0, 1, 2, \ldots, N_h -1 \NR
}
to denote the entrywise complex modulus of \m{\V {g}}, we see that the convex optimization now takes the form
\Disp {
\NC \Hat {\T {\V {g}}}
= \NC \startcases
\NC \Min {\T {\V {g}}', \V {m}} \Q
\MC \IP { \V {1}, \V {m} } \NR
\NC \Rm {subject} \; \Rm {to} \Q
\MC \VNm { \T {\M {U}}_{n_h} \T {\V {g}}' }_2
\leq \IP { \V {u}_{n_h}, \V {m} } \NR
\NC \MC \VNm { \T {\M {U}}_{n_h} \T {\M {P}}^\Adj \RB { \T {\M {P}} \T {\V {g}}' -\T {\V {y}} } }_2
\leq \g \NR
\stopcases \NR[+]
\NC n_h 
= \NC 0, 1, 2, \ldots, N_h-1 \NR
}

\stopsection

\startsection [title={Second Order Cone Programming}]

We shall see furthermore that the program can be cast into an SOCP.
To make the point clearer, we rewrite Program () into a stricter, extended block matrix form, by defining auxiliary variables with dimension

\Disp{
\NC \V{t}_i
\NC \in \MB {V}_{\MB {R}} \SB {3N_h} \NR
\NC \V{x}_i
\NC \in \MB {V}_{\MB {R}} \SB {3N_h} \NR
\NC \M{A}_i
\NC \in \MB {M}_{\MB {R}} \SB {2N_h, 3N_h} \NR
\NC \V{b}_i
\NC \in \MB {V}_{\MB {R}} \SB {3N_h} \NR
\NC \V{c}_i
\NC \in \MB {V}_{\MB {R}} \SB {2N_h} \NR
\NC d_i
\NC \in \MB {R} ^+ \NR
}

defined below to be

\Disp{
\NC \V{t}
= \NC \startTheMatrix
\NC \V{0} \NR
\NC \V{1} \NR
\stopTheMatrix \NR[+]
\NC \V{x}'
= \NC \startTheMatrix
\NC \T {\V {g}}' \NR
\NC \V{m} \NR
\stopTheMatrix \NR[+]
}

with

\Disp{
\NC \M{A}_i
= \NC \startTheMatrix
\NC \T {\M {U}}_{i}, \NC \M{0} \NR
\stopTheMatrix \NR[+]
\NC \V{b}_i
= \NC \V{0} \NR[+]
\NC \V{c}_i
= \NC \startTheMatrix
\NC \V{0} \NR
\NC \V {u}_{i} \NR
\stopTheMatrix \NR[+]
\NC d_i
= \NC 0 \NR[+]
\NC i 
= \NC 0, 1, 2, \ldots, N_h -1 \NR
}

And

\Disp{
\NC \M{A}_i
= \NC \startTheMatrix
\NC -\T {\M {U}}_{i -N_h} \T {\M {P}}^\Adj \T {\M {P}}, \NC \M{0} \NR
\stopTheMatrix \NR[+]
\NC \V{b}_i
=\NC \T {\M {U}}_{i -N_h} \T {\M {P}}^\Adj \T {\V {y}} \NR[+]
\NC \V{c}_i
= \NC \V{0} \NR[+]
\NC d_i
= \NC \g \NR[+]
\NC i 
= \NC N_h, N_h +1, N_h +2, \ldots, 2N_h -1 \NR
}

The step that we convert \m {\Hat {\V {g}}} back is same as Algorithm () is same as before, but this is listed for sake of completeness.

\Result
{Algorithm}
{
\startitemize[n]
\item Input \m{\M {P} \in \MB {M}_{\MB {C}} \SB {N_y, N_h}}, \m{\V {y} \in \MB {V}_{\MB {C}} \SB {N_y}}, \m{\g > 0}.
\item Define \m{\V{t}, \V{x}, \M{A}_i, \V{b}_i, \V{c}_i, d_i} according to ().
\item Calculate
\Disp{
\NC \Hat {\V {x}}
\LA \NC \startcases
\NC \Min {\V {x}' \in \MB {V}_{\MB {C}} \SB {N_h}}
\MC \IP {\V {t}, \V {x}'} \NR[+]
\NC \Rm {subject} \; \Rm {to}
\Q  \MC \VNm {\M {A} _{i} \V {x}' +\V{b} _{i}} _2
\leq \IP {\V{c}_{i}, \V {x}'} +\V{d}_i \NR
\NC \MC i 
=0, 1, 2, \ldots, 2N_h -1 \NR
\stopcases \NR[+]
}
\item Extract
\Disp{
\NC \Hat{\T{\V{g}}}
\LA \NC \Hat{\V{x}} \DB {0 : 2N_h-1} \NR[+]
}
\item Convert 
\Disp{
\NC \Hat{\V{g}}
\LA \NC \MC{R} ^{-1} \SB {\Hat{\T{\V{g}}}} \NR[+]
}
\item Calculate
\Disp {
\NC \Hat {G}
\LA \NC \Rm {vec}^{-1} \SB {\Hat {g}} \NR[+]
}
\item Calculate
\Disp {
\NC \Hat {\M {H}}
\LA \NC \M {K} \Hat {\M {G}} \M {K}^\Adj \NR[+]
}
\item Output \m {\Hat {\M {H}}}.
\stopitemize
}

\stopsection

\startsection [title={Simulation}]

One can convert primal-dual interior point algorithm to make the SOCP easier to calculate.
We mention, again, Boyd and Vandenberghe (2004), {\it Convex Optimization} chap.\ 11 as a standard reference, and ``\m {\ell_1}-MAGIC'' (Cand\`es and J Romberg 2005) for discussion of SOCP applied to DS.

However, for brevity we use Python Library CVXPY to solve the SOCP.

\stopsection

\startsection [title={Discussion}]

On the other hand, DS goes not without criticism for its large complexity (Friedlander and Saunders 2007).
Indeed it is clear, even from the primal-dual implementation above, that DS requires memory for large dimensional vector, but OMP does not.

And a meaningful comparison of bounds of OMP and DS is not easy, as their settings are somewhat different.
The author thus criticises the tendency of current literature to mix up results for DS and OMP without clear justification.
Indeed, DS calls for a RIP matrix and guarantees such performance even in noisy observation (Cand\`es and Tao 2005, 2007), while RIP is not easily to justify rigorously, and its easy construction is even an open problem.
On the other hand, as of OMP, besides the original justification of entrywise i.i.d.\ Gaussian sensing matrix (Tropp and Gilbert 2007a), and existent attempt to characterize MIP condition, there is much to be done OMP for Gaussian.

\stopsection

\stopchapter
